\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{werman1985,Rubner2000}
\citation{villani09optimal}
\citation{pmlr-v32-solomon14,kusnerb2015,pmlr-v70-arjovsky17a,ho2017}
\citation{frogner2015nips,panaretos2016,ebert2017ConstructionON,bigot2017,flamary2018WDA}
\citation{bonnel2011,Rubner2000,solomon2015}
\citation{klouri17,peyre2019COTnowpublisher}
\citation{leeSidford2013PathFI}
\citation{cuturinips13}
\citation{sinkhorn1967,knight2008,kalantari2008}
\citation{altschulernips17}
\citation{altschuler2018Nystrom}
\citation{xie2018proxpointOT,dvurechensky18aICML,lin2019}
\citation{blondel2018ICML,cuturi2016SIAM}
\citation{genevay2016stochOT,khalilabid2018}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{Ghaoui2010SafeFE}
\citation{blondel2018ICML}
\citation{nocedal1980,byrd1995L-BFGS-B}
\citation{flamary2017pot}
\@writefile{toc}{\contentsline {section}{\numberline {2}Regularized discrete OT}{2}{section.2}\protected@file@percent }
\newlabel{sec:regularized_discrete_ot}{{2}{2}{Regularized discrete OT}{section.2}{}}
\citation{kantorovich1942}
\citation{cuturinips13}
\citation{cuturinips13}
\citation{sinkhorn1967}
\citation{benamou2015IterativeBP}
\citation{altschulernips17}
\citation{Ghaoui2010SafeFE}
\@writefile{toc}{\contentsline {paragraph}{Sinkhorn divergence.}{3}{section*.2}\protected@file@percent }
\newlabel{monge-kantorovich}{{2}{3}{Sinkhorn divergence}{section*.2}{}}
\newlabel{sinkhorn-primal}{{1}{3}{Sinkhorn divergence}{equation.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Dual of Sinkhorn divergence.}{3}{section*.3}\protected@file@percent }
\newlabel{sinkhorn-dual}{{2}{3}{Dual of Sinkhorn divergence}{equation.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Screened dual of Sinkhorn divergence}{3}{section.3}\protected@file@percent }
\newlabel{sec:screened_dual_of_sinkhorn_divergence}{{3}{3}{Screened dual of Sinkhorn divergence}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Plots of $(e^{u^\star }, e^{v^\star })$ with $(u^\star , v^\star )$ is the pair solution of dual of Sinkhorn divergence\nobreakspace  {}\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {sinkhorn-dual}\unskip \@@italiccorr )}} and the thresholds $\alpha _u, \alpha _v$.\relax }}{3}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:motivations}{{1}{3}{Plots of $(e^{u^\star }, e^{v^\star })$ with $(u^\star , v^\star )$ is the pair solution of dual of Sinkhorn divergence~\eqref {sinkhorn-dual} and the thresholds $\alpha _u, \alpha _v$.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation.}{3}{section*.5}\protected@file@percent }
\citation{KALANTARI199687}
\citation{peyre2019COTnowpublisher}
\@writefile{toc}{\contentsline {paragraph}{Static screening test.}{4}{section*.6}\protected@file@percent }
\newlabel{screen-sinkhorn}{{3}{4}{Static screening test}{equation.3.3}{}}
\newlabel{lemma_actives_sets}{{1}{4}{}{lemma.1}{}}
\newlabel{I_epsilon_kappa_J_epsilon_kappa}{{4}{4}{}{equation.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Screening with a fixed number budget of points.}{4}{section*.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plots of $\varepsilon $ and $\kappa $ as a function of number budget of points for a screening test with $n_b=m_b$ and the parameters $\mu , \nu , \eta , C$ are set as in Figure\nobreakspace  {}\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {fig:motivations}\unskip \@@italiccorr )}}. $(\varepsilon , \kappa )$ tends to $(0,1)$ as $(n_b,m_b)$ tends to $(n,m)$.\relax }}{4}{figure.caption.8}\protected@file@percent }
\newlabel{fig:kappa_epsilon}{{2}{4}{Plots of $\varepsilon $ and $\kappa $ as a function of number budget of points for a screening test with $n_b=m_b$ and the parameters $\mu , \nu , \eta , C$ are set as in Figure~\eqref {fig:motivations}. $(\varepsilon , \kappa )$ tends to $(0,1)$ as $(n_b,m_b)$ tends to $(n,m)$.\relax }{figure.caption.8}{}}
\newlabel{epsilon_kappa}{{5}{4}{Screening with a fixed number budget of points}{equation.3.5}{}}
\citation{byrd1995L-BFGS-B}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces \textsc  {Screenkhorn}$(C,\eta ,\mu ,\nu ,n_b,m_b)$\relax }}{5}{algocf.1}\protected@file@percent }
\newlabel{screenkhorn}{{1}{5}{Screening with a fixed number budget of points}{algocf.1}{}}
\newlabel{screen-sinkhorn_second_def}{{6}{5}{Screening with a fixed number budget of points}{equation.3.6}{}}
\newlabel{prop:bounds_of_usc_and_vsc}{{1}{5}{}{proposition.1}{}}
\newlabel{bound_on_u}{{7}{5}{}{equation.3.7}{}}
\newlabel{bound_on_v}{{8}{5}{}{equation.3.8}{}}
\citation{dvurechensky18aICML}
\citation{lin2019}
\citation{flamary2017pot}
\@writefile{toc}{\contentsline {section}{\numberline {4}Theoretical analysis and guarantees}{6}{section.4}\protected@file@percent }
\newlabel{sec:analysis_of_marginal_violations}{{4}{6}{Theoretical analysis and guarantees}{section.4}{}}
\newlabel{proposition_error_in_marginals}{{2}{6}{}{proposition.2}{}}
\newlabel{marginal-error-mu}{{9}{6}{}{equation.4.9}{}}
\newlabel{marginal-error-nu}{{10}{6}{}{equation.4.10}{}}
\newlabel{prop:objective-error}{{3}{6}{}{proposition.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical experiments}{6}{section.5}\protected@file@percent }
\newlabel{sec:numerical_experiments}{{5}{6}{Numerical experiments}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Setup}{6}{subsection.5.1}\protected@file@percent }
\citation{flamary2018WDA}
\citation{courty2017optimal}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Empirical evaluation of \textsc  {Screenkhorn} vs \textsc  {Sinkhorn} for normalized cost matrix \emph  {i.e.} $\delimiter "026B30D C\delimiter "026B30D _\infty =1$. Top panel: marginal violations in relation with the budget of points. Bottom panel: (left) ratio of computation times $\frac  {T_{\textsc  {Sinkhorn}}}{T_{\text  {\textsc  {Screenkhorn}}}}$ and, (right) relative divergence variation. The results are averaged over $30$ trials.\relax }}{7}{figure.caption.10}\protected@file@percent }
\newlabel{fig:margin_expe}{{3}{7}{Empirical evaluation of \textsc {Screenkhorn} vs \textsc {Sinkhorn} for normalized cost matrix \emph {i.e.} $\norm {C}_\infty =1$. Top panel: marginal violations in relation with the budget of points. Bottom panel: (left) ratio of computation times $\frac {T_{\textsc {Sinkhorn}}}{T_{\text {\textsc {Screenkhorn}}}}$ and, (right) relative divergence variation. The results are averaged over $30$ trials.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Analysing on toy problem}{7}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Integrating \textsc  {Screenkhorn} into machine learning pipelines}{7}{subsection.5.3}\protected@file@percent }
\citation{ganin2016domain}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Wasserstein Discriminant Analysis : running time gain for a toy dataset and for MNIST as a function of the number of examples and the data decimation factor in \textsc  {Screenkhorn}.\relax }}{8}{figure.caption.11}\protected@file@percent }
\newlabel{fig:wda}{{4}{8}{Wasserstein Discriminant Analysis : running time gain for a toy dataset and for MNIST as a function of the number of examples and the data decimation factor in \textsc {Screenkhorn}.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces OT Domain adaptation : running time gain for MNIST as a function of the number of examples and the data decimation factor \textsc  {Screenkhorn}. Hyperparameter values (left) $1$. (right) $10$.\relax }}{8}{figure.caption.12}\protected@file@percent }
\newlabel{fig:otda}{{5}{8}{OT Domain adaptation : running time gain for MNIST as a function of the number of examples and the data decimation factor \textsc {Screenkhorn}. Hyperparameter values (left) $1$. (right) $10$.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{section.6}\protected@file@percent }
\bibdata{biblio}
\bibcite{khalilabid2018}{{1}{}{{}}{{}}}
\bibcite{altschuler2018Nystrom}{{2}{}{{}}{{}}}
\bibcite{altschulernips17}{{3}{}{{}}{{}}}
\bibcite{pmlr-v70-arjovsky17a}{{4}{}{{}}{{}}}
\bibcite{benamou2015IterativeBP}{{5}{}{{}}{{}}}
\bibcite{bigot2017}{{6}{}{{}}{{}}}
\bibcite{blondel2018ICML}{{7}{}{{}}{{}}}
\bibcite{bonnel2011}{{8}{}{{}}{{}}}
\bibcite{byrd1995L-BFGS-B}{{9}{}{{}}{{}}}
\bibcite{courty2017optimal}{{10}{}{{}}{{}}}
\bibcite{cuturinips13}{{11}{}{{}}{{}}}
\bibcite{cuturi2016SIAM}{{12}{}{{}}{{}}}
\bibcite{dvurechensky18aICML}{{13}{}{{}}{{}}}
\bibcite{ebert2017ConstructionON}{{14}{}{{}}{{}}}
\bibcite{fei2014-GPU-L-BFGS-B}{{15}{}{{}}{{}}}
\bibcite{flamary2017pot}{{16}{}{{}}{{}}}
\bibcite{flamary2018WDA}{{17}{}{{}}{{}}}
\bibcite{frogner2015nips}{{18}{}{{}}{{}}}
\bibcite{ganin2016domain}{{19}{}{{}}{{}}}
\bibcite{genevay2016stochOT}{{20}{}{{}}{{}}}
\bibcite{Ghaoui2010SafeFE}{{21}{}{{}}{{}}}
\bibcite{ho2017}{{22}{}{{}}{{}}}
\bibcite{kalantari2008}{{23}{}{{}}{{}}}
\bibcite{KALANTARI199687}{{24}{}{{}}{{}}}
\bibcite{kantorovich1942}{{25}{}{{}}{{}}}
\bibcite{knight2008}{{26}{}{{}}{{}}}
\bibcite{klouri17}{{27}{}{{}}{{}}}
\bibcite{kusnerb2015}{{28}{}{{}}{{}}}
\bibcite{leeSidford2013PathFI}{{29}{}{{}}{{}}}
\bibcite{lin2019}{{30}{}{{}}{{}}}
\bibcite{nocedal1980}{{31}{}{{}}{{}}}
\bibcite{panaretos2016}{{32}{}{{}}{{}}}
\bibcite{peyre2019COTnowpublisher}{{33}{}{{}}{{}}}
\bibcite{Rubner2000}{{34}{}{{}}{{}}}
\bibcite{sinkhorn1967}{{35}{}{{}}{{}}}
\bibcite{solomon2015}{{36}{}{{}}{{}}}
\bibcite{pmlr-v32-solomon14}{{37}{}{{}}{{}}}
\bibcite{villani09optimal}{{38}{}{{}}{{}}}
\bibcite{werman1985}{{39}{}{{}}{{}}}
\bibcite{xie2018proxpointOT}{{40}{}{{}}{{}}}
\bibstyle{plain}
\citation{khalilabid2018}
\@writefile{toc}{\contentsline {section}{\numberline {7}Supplementary material}{12}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Proof of Lemma\nobreakspace  {}\ref  {lemma_actives_sets}}{12}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Proof of Proposition\nobreakspace  {}\ref  {prop:bounds_of_usc_and_vsc}}{12}{subsection.7.2}\protected@file@percent }
\newlabel{upper_bound_v_potential}{{11}{12}{Proof of Proposition~\ref {prop:bounds_of_usc_and_vsc}}{equation.7.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Proof of Proposition\nobreakspace  {}\ref  {proposition_error_in_marginals}}{12}{subsection.7.3}\protected@file@percent }
\newlabel{lem:pinsker}{{2}{13}{}{lemma.2}{}}
\newlabel{i-th-marginal-mu}{{12}{13}{Proof of Proposition~\ref {proposition_error_in_marginals}}{equation.7.12}{}}
\newlabel{i-th-marginal-nu}{{13}{13}{Proof of Proposition~\ref {proposition_error_in_marginals}}{equation.7.13}{}}
\newlabel{rem:orders_of_epsilonappa}{{1}{14}{}{remark.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Proof of Proposition\nobreakspace  {}\ref  {prop:objective-error}}{14}{subsection.7.4}\protected@file@percent }
\newlabel{priori_bounds_induction}{{14}{15}{Proof of Proposition~\ref {prop:objective-error}}{equation.7.14}{}}
\citation{lin2019}
\newlabel{induction_uppers_bounds}{{15}{16}{Proof of Proposition~\ref {prop:objective-error}}{equation.7.15}{}}
\newlabel{R_constant}{{16}{16}{Proof of Proposition~\ref {prop:objective-error}}{equation.7.16}{}}
\newlabel{post_bounds_induction_2}{{17}{16}{Proof of Proposition~\ref {prop:objective-error}}{equation.7.17}{}}
\citation{fei2014-GPU-L-BFGS-B}
\citation{altschulernips17}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{lemma_bounds_on_marginals}{{3}{17}{}{lemma.3}{}}
\newlabel{l1-norm-mu-sc}{{18}{17}{}{equation.7.18}{}}
\newlabel{l1-norm-nu-sc}{{19}{17}{}{equation.7.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Additional experimental results}{17}{subsection.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experimental setup.}{17}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{On the use of a constrained L-BFGS-B solver.}{17}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison with other solvers.}{17}{section*.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Time gain \textsc  {Screenkhorn} and \textsc  {Greenkhorn} \relax }}{17}{figure.caption.18}\protected@file@percent }
\newlabel{fig:screenkhorn_greenkhorn_gain}{{6}{17}{Time gain \textsc {Screenkhorn} and \textsc {Greenkhorn} \relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Accuracy of a $1$-nearest-neighbour after WDA for the (left) toy problem and, (right) MNIST). We note a slight loss of performance for the toy problem, whereas for MNIST, all approaches yield the same performance. \relax }}{18}{figure.caption.19}\protected@file@percent }
\newlabel{fig:wda_gain}{{7}{18}{Accuracy of a $1$-nearest-neighbour after WDA for the (left) toy problem and, (right) MNIST). We note a slight loss of performance for the toy problem, whereas for MNIST, all approaches yield the same performance. \relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces OT Domain Adaptation on a 3-class Gaussian toy problem. (top-left) Examples of source and target samples. (top-right) Evolution of the accuracy of a 1-nearest-neighbour classifier with respect to the number of samples. (bottom-left) Running time of the \textsc  {Sinkhorn} and \textsc  {Screenkhorn} for different decimation factors. (bottom-right). Gain in computation time. This toy problem is a problem in which classes are overlapping and distance between samples are rather limited. According to our analysis, this may be a situation in which \textsc  {Screenkhorn} may result in smaller computational gain. We can remark that with respect to the accuracy \textsc  {Screenkhorn} with decimation factors up to $10$ are competitive with \textsc  {Sinkhorn}, although a slight loss of performance. Regarding computational time, for this example, small decimation factors does not result in gain. However for above $5$-factor decimation, the gain goes from $2$ to $10$ depending on the number of samples. \relax }}{18}{figure.caption.20}\protected@file@percent }
\newlabel{fig:otda:extra}{{8}{18}{OT Domain Adaptation on a 3-class Gaussian toy problem. (top-left) Examples of source and target samples. (top-right) Evolution of the accuracy of a 1-nearest-neighbour classifier with respect to the number of samples. (bottom-left) Running time of the \textsc {Sinkhorn} and \textsc {Screenkhorn} for different decimation factors. (bottom-right). Gain in computation time. This toy problem is a problem in which classes are overlapping and distance between samples are rather limited. According to our analysis, this may be a situation in which \textsc {Screenkhorn} may result in smaller computational gain. We can remark that with respect to the accuracy \textsc {Screenkhorn} with decimation factors up to $10$ are competitive with \textsc {Sinkhorn}, although a slight loss of performance. Regarding computational time, for this example, small decimation factors does not result in gain. However for above $5$-factor decimation, the gain goes from $2$ to $10$ depending on the number of samples. \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces OT Domain adaptation MNIST to USPS : (top) Accuracy and (bottom) running time of \textsc  {Sinkhorn} and \textsc  {Screenkhorn} for hyperparameter of the $\ell _{p,1}$ regularizer (left) $\lambda = 1$ and (right) $\lambda =10$. Note that this value impacts the ground cost of each Sinkhorn problem involved in the iterative algorithm. The accuracy panels also report the performance of a $1$-NN when no-adaptation is performed. We remark that the strenght of the class-based regularization has influence on the performance of \textsc  {Screenkhorn} given a decimation factor. For small value on the left, \textsc  {Screenkhorn} slightly performs better than \textsc  {Sinkhorn}, while for large value, some decimation factors leads to loss of performances. Regarding, running time, we can note that \textsc  {Sinkhorn} is far less efficient than \textsc  {Screenkhorn} with an order of magnitude for intermediate number of samples. \relax }}{19}{figure.caption.21}\protected@file@percent }
\newlabel{fig:otda:mnist:extra}{{9}{19}{OT Domain adaptation MNIST to USPS : (top) Accuracy and (bottom) running time of \textsc {Sinkhorn} and \textsc {Screenkhorn} for hyperparameter of the $\ell _{p,1}$ regularizer (left) $\lambda = 1$ and (right) $\lambda =10$. Note that this value impacts the ground cost of each Sinkhorn problem involved in the iterative algorithm. The accuracy panels also report the performance of a $1$-NN when no-adaptation is performed. We remark that the strenght of the class-based regularization has influence on the performance of \textsc {Screenkhorn} given a decimation factor. For small value on the left, \textsc {Screenkhorn} slightly performs better than \textsc {Sinkhorn}, while for large value, some decimation factors leads to loss of performances. Regarding, running time, we can note that \textsc {Sinkhorn} is far less efficient than \textsc {Screenkhorn} with an order of magnitude for intermediate number of samples. \relax }{figure.caption.21}{}}
