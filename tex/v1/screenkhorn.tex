\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%\usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[preprint]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

%--------------------------------------------------------------------------------------------------------------------------------% 

\RequirePackage{mathrsfs, amsthm, amsmath, amsfonts, amssymb, mathtools}%
\usepackage[titletoc,title]{appendix}%
\usepackage{dsfont}%
\usepackage{caption} % 
\usepackage{subcaption}
\usepackage{float}%
\usepackage{graphicx, color}% 
\usepackage{bm}%bold equations

%% FORMATTING APPNENDICES
\usepackage{etoolbox}
\patchcmd{\appendices}{\quad}{. }{}{}

%% TABLE
\usepackage{pgfplots}%
\usepackage{booktabs,multirow,array,multicol}
\newcommand{\otoprule}{\midrule[\heavyrulewidth]}

%% ALGORITHM
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

%% SECTIONS
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}%[section]
\newtheorem*{notation}{Notation}%[section]
\newtheorem{discus}{Discussion}%[section]
\newtheorem{remark}{Remark}%[section]
\newtheorem{example}{Example}%[section]
\newtheorem{exs}{Examples}%[section]
\newtheorem{ca}{Cas}
\newtheorem{remarks}{Remarks}%[section]
\newtheorem{assumption}{Assumption}{\bf}{\rm}%

% MATH MACROS 

\usepackage{relsize}
\newcommand*{\defeq}{\stackrel{\mathsmaller{\mathsf{def.}}}{=}}

\newcommand{\ba}{\mathbf{a}}%
\newcommand{\bb}{\mathbf{b}}%

\newcommand{\bC}{{\mathbf C}}%
\newcommand{\bP}{{\mathbf P}}%
\newcommand{\bU}{{\mathbf U}}%

\newcommand{\cX}{\mathcal X}%
\newcommand{\cY}{\mathcal Y}%
\newcommand{\cM}{\mathcal M}%
\newcommand{\cC}{\mathcal C}%

\newcommand{\td}{\text d}%
\newcommand*{\argminl}{\argmin\limits}%
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\sumin}{\sum_{i=1}^n}
\newcommand{\sumip}{\sum_{j=1}^m}
\newcommand{\intt}{\int_{0}^{t}}
\newcommand{\inttau}{\int_{0}^{\gamma}}
\newcommand{\I}{1\!\mbox{I}}
\newcommand{\e}{{\mathrm e}}
\newcommand{\diff}{\mathrm d}
\newcommand{\noi}{\noindent}
\newcommand{\eps}{\varepsilon}
\usepackage{datenumber}
\newcommand{\sptr}[2]{\langle #1 | #2 \rangle}
\newcommand{\inr}[1]{\langle #1 \rangle}
\newcommand{\inrbig}[1]{\bigg\langle #1 \bigg\rangle}
\newcommand{\ind}[1]{{\mathds{1}}_{{#1}}}%
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\E}{\mathds{E}}
\newcommand{\V}{\mathds{V}}
\newcommand{\Lim}{\displaystyle \lim\limits}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\cD}{\mathcal D}
\newcommand{\cE}{\mathcal E}
\newcommand{\cT}{\mathcal T}
\newcommand{\bA}{{\boldsymbol A}}%
\newcommand{\bcA}{\boldsymbol{\mathcal{C}}}%
\newcommand{\bB}{{\boldsymbol B}}%
\newcommand{\bcB}{\boldsymbol{\mathcal{B}}}%
\newcommand{\bD}{{\boldsymbol D}}%
\newcommand{\bE}{{\boldsymbol E}}%
\newcommand{\bF}{{\boldsymbol F}}%
\newcommand{\bG}{{\boldsymbol G}}%
\newcommand{\bH}{{\boldsymbol H}}%
\newcommand{\bI}{{\boldsymbol I}}%
\newcommand{\bJ}{{\boldsymbol J}}%
\newcommand{\bK}{{\boldsymbol K}}%
\newcommand{\bL}{{\boldsymbol L}}%
\newcommand{\bM}{{\boldsymbol M}}%
\newcommand{\bN}{{\boldsymbol N}}%
\newcommand{\bO}{\boldsymbol 0}%
\newcommand{\bQ}{\boldsymbol Q}%
\newcommand{\bR}{\boldsymbol R}%
\newcommand{\cS}{{\mathcal{S}}}%
\newcommand{\bcQ}{\boldsymbol{\mathcal{Q}}}%
\newcommand{\bV}{\boldsymbol V}%
\newcommand{\bW}{\boldsymbol W}%
\newcommand{\bX}{\boldsymbol  X}%
\newcommand{\bT}{\boldsymbol  T}%
\newcommand{\bcE}{\boldsymbol{\mathcal{E}}}%
\newcommand{\bcH}{\boldsymbol{\mathcal{H}}}%
\newcommand{\bcX}{\boldsymbol{\mathcal{X}}}%
\newcommand{\bcJ}{\boldsymbol{\mathcal{J}}}%
\newcommand{\bcY}{\boldsymbol{\mathcal{Y}}}%
\newcommand{\bcW}{\boldsymbol{\mathcal{W}}}%
\newcommand{\bcT}{\boldsymbol{\mathcal{T}}}%
\newcommand{\bcM}{\boldsymbol{\mathcal{M}}}%
\newcommand{\bcZ}{\boldsymbol{\mathcal{Z}}}%
\newcommand{\bcO}{\boldsymbol{\mathcal{O}}}%
\newcommand{\bcG}{\boldsymbol{\mathcal{G}}}%
\newcommand{\bcU}{\boldsymbol{\mathcal{U}}}%
\newcommand{\bcV}{\boldsymbol{\mathcal{V}}}%
\newcommand{\bcR}{\boldsymbol{\mathcal{R}}}%
\newcommand{\bcI}{\boldsymbol{\mathcal{I}}}%
\newcommand{\by}{\boldsymbol y}%
\newcommand{\bu}{\boldsymbol u}%
\newcommand{\bv}{\boldsymbol  v}%
\newcommand{\bz}{\boldsymbol z}%
\newcommand{\bY}{\boldsymbol{Y}}%
\newcommand{\bZ}{\boldsymbol Z}
\newcommand{\bg}{\boldsymbol g}%
\newcommand{\beps}{\boldsymbol \varepsilon}%
\newcommand{\bSigma}{\boldsymbol \Sigma}%
\newcommand{\blambda}{\boldsymbol  \lambda}
\newcommand{\bbeta}{\boldsymbol  \beta}%
\newcommand{\bLambda}{\boldsymbol  \Lambda}%
\newcommand{\bTheta}{\boldsymbol  \Theta}
\newcommand{\bDelta}{\boldsymbol{\Delta}}%
\newcommand{\bXi}{\boldsymbol{\Xi}}%
\newcommand{\ov}{\overrightarrow}%
\newcommand{\cP}{\mathcal P}%
\newcommand{\cQ}{\mathcal Q}%
\renewcommand{\P}{\mathds{P}}
\newcommand{\bctheta}{\boldsymbol{\mathcal{\Theta}}}%
\newcommand{\btheta}{\boldsymbol  \Theta}%
\newcommand{\bcdot}{\raisebox{-0.80ex}{\scalebox{1.8}{$\cdot$}}}
\newcommand{\cst}{\raisebox{-0.15ex}{\scalebox{1.30}{$c$}}}
\newcommand{\Cst}{\raisebox{-0.15ex}{\scalebox{1.10}{$C$}}}
\newcommand{\varsig}{\raisebox{-0.15ex}{\scalebox{1.30}{$\varsigma$}}}
\usepackage{accents}
\newcommand*{\dt}[1]{\accentset{\raisebox{0ex}{$\,\star$}}{#1}}
\newcommand*{\ddt}[1]{\accentset{\raisebox{0ex}{$\,\,\star$}}{#1}}

\newcommand*{\fstderiv}[1]{\accentset{\mbox{\Large\bfseries .}}{#1}}
\newcommand*{\scdderiv}[1]{\accentset{\mbox{\Large\bfseries .\hspace{-0.25ex}.}}{#1}}

%% MATH OPERATORS
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

%--------------------------------------------------------------------------------------------------------------------------------%

\title{Screening Sinkhorn Algorithm via Dual Projections}

\begin{document}

\maketitle

\begin{abstract}
Computing optimal transport distances, such as the earth mover's distance is a fundamental problem in machine learning, statistics, and computer vision. 
% Despite the recent several introduction of several algorithms with good empirical performance, it is unknown whether general optimal transport distances can be approximated in near-linear time.
% This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhron divergence distance.
% This result relies on a new analysis of Sinkhorn iterations, which also directly suggests a new greedy coordinate descent algorithm GREENKHORN with the same theoretical  guarantees. 
% Numerical simulations illustrate that GREENKHORN significantly outperforms the classical SINKHORN Algorithm in practice.
\end{abstract}

\section{Introduction}


\paragraph{Related work.}

\paragraph{Our contribution.}


\paragraph{Notation.}

We denote $\Sigma_n$ the probability simplex with $n$ bins, namely the set of probability vectors in $\R_+^n$, i.e., $\Sigma_n = \{w \in \R_+^n: \sum_{i=1}^n w_i = 1\}.$
For any positive matrix $T \in \R^{n\times m}$, we define its negative entropy as $H(T) = -\sum_{i,j} T_{ij} \log(T_{ij}).$
Let $r(T) = T\mathbf 1_m \in \R^n$ and $c(T) = T^\top\mathbf 1_n \in \R^m$ denote the rows and columns sums of $T$ respectively. The coordinates $r_i(T)$ and $c_j(T)$ denote the $i$-th row sum and the $j$-th column sum of $T$, respectively.
The scalar product between two matrices denotes the usual inner product, that is $\inr{T, W} = \text{tr}(T^\top W) = \sum_{i,j}T_{ij}W_{ij},$ where $T^\top$ is the transpose of $T$. 
We write $\mathbf{1}$ (resp. $\mathbf{0}$) the vector having all coordinates equal to one (resp. zero).
$\Delta(w)$ denotes the diag operator, such that if $w \in \R^n$, then $\Delta(w) = \text{diag}(w_1, \ldots, w_n)\in \R^{n\times n}$.
Throughout this paper, when applied to matrices and vectors,  $\odot$ and $\oslash$ (Hadamard product and division) and exponential notations refer to elementwise operators.
We also denote $|I|$ the cardinality of a finite set $I$.
Given two real numbers $a$ and $b$, we write $a\vee b = \max(a,b)$ and $a\wedge b = \min(a,b).$
% We denote by $\iota_C$ the indicator function of a convex set $C$, that is,
% \begin{equation*}
%   \iota_{C}(w) = \begin{cases}
%             0, &\text{ if } w\in C,\\
%             \infty, &\text{otherwise}.
%               \end{cases}
% \end{equation*}


\section{Regularized optimal transport}
\label{sec:optimal_transport}

 We briefly present in this section the setup of optimal transport between two discrete measures. We then consider the case when those distributions are only available through a finite number of samples, that is $\mu = \sum_{i=1}^n \mu_i \delta_{x_i} \in \Sigma_n$ and $\nu = \sum_{j=1}^m \nu_i \delta_{y_j} \in \Sigma_m$. 
We denote their probabilistic couplings set as $\Pi(\mu, \nu) = \{P \in \R_+^{n\times m}, P\mathbf{1}_m = \mu, P^\top \mathbf{1}_n = \nu\}.$ 
\paragraph{Sinkhorn divergence.}

 % Roughly speaking, the optimal transport aims at computing a minimal cost transportation between the two measures $\mu$ and $\nu$
Approximating the optimal transport distance between the two measures $\mu$ and $\nu$  amounts to solving a linear problem given by~\cite{kantorovich1942}
\begin{equation}
  \label{monge-kantorovich}
  \mathcal{S}(\mu, \nu) =  \min_{P\in \Pi(\mu, \nu)} \inr{M, P},
\end{equation}
where $P= (P_{ij}) \in \R^{n\times m}$ is called the transportation plan, namely each entry $P_{ij}$ represents the fraction of mass moving from $x_i$ to $y_j$, and $M= (M_{ij}) \in \R^{n\times m}$ is a cost matrix comprised of nonnegative elements related to the energy needed to move a probability mass from $x_i$ to $y_j$. 
The entropic regularization of optimal transport distances~\citep{cuturinips13} relies on the addition of a penalty term as follows:
\begin{equation}
\label{sinkhorn-primal}
  \mathcal{S}_\eta(\mu, \nu) =  \min_{P\in \Pi(\mu, \nu)} \{\inr{M, P} - \eta H(P)\},
\end{equation}
where $\eta > 0$ is a regularization parameter. We refer to $\mathcal{S}_\eta(\mu, \nu) $ as the \emph{Sinkhorn divergence}~\citep{cuturinips13}.

\paragraph{Dual of Sinkhorn divergence.}

Below we provide the derivation of the dual problem for the regularized optimal transport problem~\eqref{sinkhorn-primal}. Towards this end, we begin with writing its Lagrangian dual function :
\begin{equation*}
  \mathscr{L}(P,y, z) = \inr{M,P} + \eta \inr{\log P, P} + \inr{y, P\mathbf{1}_m - \mu} + \inr{z,P^\top \mathbf{1}_n - \nu},
\end{equation*}
which can be rewritten in the following form:
\begin{equation*}
  \mathscr{L}(P,y, z) = - \inr{y, \mu} - \inr{z, \nu} + \inr{P, M + \eta \log P} + \inr{y, P\mathbf{1}_m} + \inr{z, P^\top \mathbf{1}_n}.
\end{equation*}
The dual of Sinkhorn divergence can be derived by solving $\min_{P \in \R_+^{n\times m}}\mathscr{L}(P,y, z)$. It is easy to check that objective function $P\mapsto \mathscr{L}(P,y, z)$ is strongly convex and differentiable. Hence, one can solve the latter minimum by setting $\nabla_P \mathscr{L}(P,y, z)$ to $\mathbf{0}_{n\times m}$. Therefore, we get 
\begin{equation}
  P^\star_{ij} = \exp\Big(- \frac{1}{\eta} (y_i + z_j + M_{ij}) - 1\Big), 
\end{equation}
for all $i=1, \ldots, n$ and $j=1, \ldots, m$. Plugging this solution, one has 
\begin{equation*}
  \min_{P \in \R_+^{n\times m}}\mathscr{L}(P,y, z) = - \inr{y, \mu} - \inr{z, \nu} - \eta \sum_{i,j} \exp\Big(- \frac{1}{\eta} (y_i + z_j + M_{ij}) - 1\Big).
\end{equation*}
Setting the change of variables $u = -y/\eta - 1/2$ and $v = - z/\eta - 1/2$, we get 
\begin{equation*}
  \min_{P \in \R_+^{n\times m}}\mathscr{L}(P,y, z) = \eta \Big( \inr{u, \mu} + \inr{v, \nu} - \sum_{i,j} \exp\big(u_i + v_j - M_{ij}/\eta)\big) + 1\Big).
\end{equation*}
Recall that the dual problem is given by $\max_{y \in \R^n, z \in \R^m} \min_{P\in \R_+^{n\times m}}\mathscr{L}(P,y, z)$, that is 
\begin{equation*}
\max_{u \in \R^n, v\in\R^m} \Big\{\eta \Big(\inr{u, \mu} + \inr{v, \nu} - \sum_{i,j} \exp\big(u_i + v_j - M_{ij}/\eta)\big) + 1\Big)\Big\},
\end{equation*}
which is equivalent to solving 
\begin{equation*}
\max_{u \in \R^n, v\in\R^m} \Big\{\inr{u, \mu} + \inr{v, \nu} - \sum_{i,j} \exp\big(u_i + v_j - M_{ij}/\eta)\big) \Big\}
\end{equation*}
and which can be rewritten in the following matrix form:
% \begin{equation*}
% \max_{u \in \R^n, v\in\R^m} \big\{\inr{u, \mu} + \inr{v, \nu} - \mathbf{1}_n^\top B(u,v)\mathbf{1}_m\big\},
% \end{equation*}
\begin{equation}
\label{sinkhorn-dual}
\min_{u \in \R^n, v\in\R^m}\big\{\Psi(u,v):= \mathbf{1}_n^\top B(u,v)\mathbf{1}_m - \inr{u, \mu} - \inr{v, \nu} \big\},
\end{equation}
where $B(u,v) = \Delta(e^{u}) K \Delta(e^{v})$ and $K = e^{-M/\eta}$ stands for the Gibbs kernel associated to the cost matrix $M$. 
% Hence, 
% \begin{equation}
% \label{sinkhorn-dual}
% \min_{u \in \R^n, v\in\R^m}\big\{\Psi(u,v):= \mathbf{1}_n^\top B(u,v)\mathbf{1}_m - \inr{u, \mu} - \inr{v, \nu} \big\},
% \end{equation}
We refer to problem~\eqref{sinkhorn-dual} to the \emph{dual of Sinkhorn divergence}.
Therefore, the optimal solution of Sinkhorn divergence is given by $P^\star = \Delta(e^{u^\star}) K \Delta(e^{v^\star})$
where the couple $(u^\star, v^\star)$ satisfies:
\begin{align*}
\label{sinkhorn-dual}
  (u^\star, v^\star) &= \argmin_{u \in \R^{n}, v\in \R^m} \{\Psi(u,v)\}.
\end{align*}
Note that the matrices $\Delta(e^{u^\star})$ and $\Delta(e^{v^\star})$ are unique up to a constant factor~\citep{sinkhorn1967}.

\section{Screened dual of Sinkhorn divergence}

For a fixed $\varepsilon > 0$ and $\kappa > 0$ we define an \emph{approximate dual of Sinkhorn divergence} 
\begin{equation} 
\label{screen-sinkhorn}
\min_{u \in \mathcal{C}^n_{\frac \varepsilon \kappa}, v\in \mathcal{C}^m_{\varepsilon\kappa}} \big\{\Psi_{\kappa}(u,v):= \mathbf{1}_n^\top B(u,v)\mathbf{1}_m - \inr{\kappa u, \mu} - \inr{\kappa^{-1}v, \nu} \big\},
\end{equation}
where $\mathcal{C}^r_{\alpha} \subseteq \R^r$, for $r\in \mathbb N$ and $\alpha >0$, is a convex set  given by $\mathcal{C}^r_{\alpha} = \{w \in \R^{r}: \min_{1\leq i\leq r} e^{w_i} \geq \alpha\}$.

The $\kappa$-parameter in problem~\eqref{screen-sinkhorn} plays a role of scaling factor, namely it allows to get a closed order of the potential vectors $e^u$ and $e^v$, while the $\varepsilon$-parameter acts like a threshold for $e^u$ and $e^v$.
Note that the setting of $\varepsilon=0$ and $\kappa=1$, the approximate dual of Sinkhorn divergence coincides with the dual of Sinkhorn divergence~\eqref{sinkhorn-dual}.

The screening procedure presented in this work is based on constructing two \emph{active sets} $I_{\varepsilon, \kappa}$ and $J_{\varepsilon, \kappa}$ throughout the dual problem of~\eqref{screen-sinkhorn} in the following way:

\begin{lemma}
\label{lemma_actives_sets}
Let $(u^{*}, v^{*})$ be an optimal solution of the problem~\eqref{screen-sinkhorn}. 
Define
\begin{equation*}
\label{I_c_epsilon}
I_{\varepsilon,\kappa} = \Big\{i=1, \ldots, n: \mu_i \geq {\varepsilon^2} \kappa^{-1} r_i(K)\Big\}, J_{\varepsilon,\kappa} = \Big\{j=1, \ldots, m: \nu_j \geq \kappa{\varepsilon^2}{} c_j(K)\Big\}
\end{equation*}
and $I^\complement_{\varepsilon,\kappa} = \{1, \ldots, n\} \backslash I_{\varepsilon, \kappa},$ and $J^\complement_{\varepsilon,\kappa} = \{1, \ldots, n\} \backslash J_{\varepsilon, \kappa}.$
Then one has $e^{u^{*}_i} = \varepsilon\kappa^{-1}$ and $e^{v^{*}_j} = \varepsilon\kappa$ for all $i \in I^\complement_{\varepsilon,\kappa} $ and $j\in J^\complement_{\varepsilon,\kappa} .$
\end{lemma}

\begin{proof}

% The dual of problem~\eqref{screen-sinkhorn} reads as 
% \begin{align*} 
% &\max_{\lambda \succeq 0, \beta \succeq 0} \min_{u \in \R^n, v\in \R^m} \big\{\mathbf{1}_n^\top B(u,v) \mathbf{1}_m - \inr{u, \mu} - \inr{v, \nu} + \inr{\lambda, \frac \varepsilon\kappa \mathbf{1}_n - e^{u}} + \inr{\beta, c \varepsilon\mathbf{1}_m - e^{v}}\big\}\\
% &=\max_{\lambda \succeq 0, \beta \succeq 0} \big\{\frac \varepsilon\kappa \inr{\lambda, \mathbf{1}_n} + c\varepsilon\inr{\beta, \mathbf{1}_m}\\
% & \qquad\qquad\qquad + \min_{u \in \R^n, v\in \R^m} \big\{\mathbf{1}_n^\top B(u,v) \mathbf{1}_m - \inr{u, \kappa\mu} - \inr{v, \frac \nu c} - \inr{\lambda,e^{u}} - \inr{\beta,e^{v}}\big\}\big\}. 
% \end{align*}
Introducing two dual variables $\lambda \in \R^n_{+}$ and $\beta \in \R^m_{+}$ for each constraint, the Lagrangian of problem~\eqref{screen-sinkhorn} reads as 
\begin{equation*}
  \mathscr{L}(u,v, \lambda, \beta) = \frac \varepsilon\kappa\inr{\lambda, \mathbf{1}_n} + \varepsilon\kappa\inr{\beta, \mathbf{1}_m} + \mathbf{1}_n^\top B(u,v) \mathbf{1}_m - \inr{\kappa u, \mu} - \inr{\kappa^{-1}v, \nu} -\inr{\lambda,e^{u}} - \inr{\beta,e^{v}}
\end{equation*}
First order conditions~\citep{boyd2004} then yield that the Lagrangian multiplicators solutions $\lambda^{*}$ and $\beta^{*}$ satisfy the following:
\begin{align*}
  &\nabla_u\mathscr{L}(u^{*},v^{*}, \lambda^{*}, \beta^{*})=  e^{u^{*}} \odot(Ke^{v^{*}} - \lambda^{*}) - \kappa\mu = \mathbf 0_n,\\
  & \text{ and } \nabla_v\mathscr{L}(u^{*},v^{*}, \lambda^{*}, \beta^{*})=  e^{v^{*}} \odot(K^\top e^{u^{*}} - \beta) - \kappa^{-1} \nu = \mathbf 0_m
\end{align*}
which leads to 
\begin{align*}
  &\lambda^{*} = K e^{v^{*}} - \kappa\mu \oslash e^{u^{*}} \text{ and }
  \beta^{*} = K^\top e^{u^{*}} - \nu \oslash \kappa e^{v^{*}}
\end{align*}
% We come up with the following two cases for solving an optimal $(u^{*},v^{*})$ of problem~\eqref{screen-sinkhorn}
% \begin{itemize}
% \item If $\lambda^{*}_i > 0$ then $e^{u^{*}_i} =\frac \varepsilon\kappa$. Then $\lambda_i = (K e^{v^{*}})_i - \frac{ c \mu_i}{\varepsilon} > 0.$
% \item  If $e^{u^{*}_i} > \frac \varepsilon\kappa$ then $\lambda^{*}_i = 0$. It implies that $e^{u^{*}_i} (Ke^{v^{*}})_i =  \kappa\mu_i$ ( ``nearly'' original Sinkhorn marginal conditions).
% \end{itemize}

% Analogously,
% \begin{itemize}
% \item If $\beta^{*}_j > 0$ and $e^{v^{*}_j} = \varepsilon\kappa$. Then $\beta_j = (K^\top e^{v^{*}})_j - \frac{\nu_j}{\varepsilon\kappa} > 0.$
% \item  If $\beta^{*}_j = 0$ and $e^{v^{*}_j} > \varepsilon\kappa$. Then $e^{v^{*}_j} (K^\top e^{u^{*}})_j =  \frac{\nu_j}{c}$ (``nearly'' original Sinkhorn marginal conditions).
% \end{itemize}

For all $i=1, \ldots, n$ we have that $e^{u^{*}_i} \geq \frac \varepsilon\kappa$. By the KKT optimality conditions, the condition on the dual variable $\lambda^{*}_i > 0$  ensures that $e^{u^{*}_i} = \varepsilon\kappa^{-1}$ and hence $i \in I^\complement_{\varepsilon,\kappa}$. Further, $\lambda^{*}_i > 0$ is equivalent to $e^{u^{*}_i}r_i(K) e^{v^{*}_j} >  \kappa{\mu_i}$ which  is satisfied when $\varepsilon^2r_i(K) >  \kappa{\mu_i}.$  
In a symmetric way we can prove the same statement for $e^{v^{*}_j}$.
\end{proof}

It is worth to note that if $e^{u^{*}_i} > \varepsilon\kappa^{-1}$ then $e^{u^{*}_i} (Ke^{v^{*}})_i =  \kappa\mu_i$ which corresponds to one of the original Sinkhorn marginal conditions up to the scaling factor $\kappa$. 


\paragraph{Screening with a fixed budget number of points.}

Recall that the approximate dual of Sinkhorn divergence is defined with respect to the parameters $\varepsilon$ and $\kappa$. 
The explicit determination of its values depends on a fixed budget numbers of points, to be chosen in a priori way, in problem~\eqref{screen-sinkhorn}.  
In the sequel of the paper, we denote by $n_b \in\{1, \ldots, n\}$ and the $m_b\in\{1, \ldots, m\}$ the budget number of points to be given for resolving problem~\eqref{screen-sinkhorn}. 
Towards this end, let us define $\xi \in \R^n$ and $\zeta \in \R^m$ to be the ordered decreasing vectors of $\mu \oslash r(K)$ and $\nu \oslash c(K)$ respectively, that is $\xi_1 \geq \xi_2 \geq \cdots \geq \xi_n$ and $\zeta_1 \geq \zeta_2 \geq \cdots \geq \zeta_m$.
To keep only a budget of $n_b$ and $m_b$ points, the parameters $\kappa$ and $\varepsilon$ satisfy ${\varepsilon^2}\kappa^{-1} = \xi_{n_b}$ and $\varepsilon^2\kappa = \zeta_{m_b}$. Hence 
\begin{equation}
\label{epsilon_kappa}
 \varepsilon = (\xi_{n_b}\zeta_{m_b})^{1/4} \text{ and } \kappa = \sqrt{\frac{\zeta_{m_b}}{\xi_{n_b}}},
\end{equation}
we then obtain $|I_{\varepsilon, \kappa}| = n_b$ and $|J_{\varepsilon, \kappa}| = m_b$.  %Using~\eqref{epsilon_kappa} we obtain
Using the previous analysis, we know, in a posterior way, that any solution $(u^*, v^*)$ of problem~\eqref{screen-sinkhorn} should satisfy: $e^{u^*_i} \geq \varepsilon\kappa^{-1}$ and $e^{v^*_j} \geq \varepsilon\kappa$ for all $(i,j) \in (I_{\varepsilon,\kappa}\times J_{\varepsilon,\kappa}),$ and $e^{u^*_i} = \varepsilon\kappa^{-1}$ and $e^{v^*_j} = \varepsilon\kappa$ for all $(i,j) \in (I^\complement_{\varepsilon,\kappa}\times J^\complement_{\varepsilon,\kappa})$
% \begin{itemize}
% 	\item $e^{u^*_i} \geq \frac \varepsilon\kappa$ and $e^{v^*_j} \geq \varepsilon\kappa$ for all $(i,j) \in (I_{\varepsilon,\kappa}\times J_{\varepsilon,\kappa}),$ and $e^{u^*_i} = \frac \varepsilon\kappa$ and $e^{v^*_j} = \varepsilon\kappa$ for all $(i,j) \in (I^\complement_{\varepsilon,\kappa}\times J^\complement_{\varepsilon,\kappa}).$
% 	\item $e^{u^*_i} = \frac \varepsilon\kappa$ and $e^{v^*_j} = \varepsilon\kappa$ for all $(i,j) \in (I^\complement_{\varepsilon,\kappa}\times J^\complement_{\varepsilon,\kappa}).$
% \end{itemize}

Basing on that facts we ``screen'' the feasibility domain in problem~\eqref{screen-sinkhorn} to the following one:
\begin{align}
\label{screen-sinkhorn_second_def}
\min\{\Psi_{\varepsilon, \kappa}(u,v)\} \text { subject to } 
\begin{cases}
e^{u_i} \geq \varepsilon\kappa^{-1}, \text{ for all } i \in I_{\varepsilon,\kappa} \text{ and } e^{u_i} = \varepsilon\kappa^{-1} \text{ for all } i \in I^\complement_{\varepsilon,\kappa}\\
e^{v_j} \geq \varepsilon\kappa, \text{ for all } j \in J_{\varepsilon,\kappa} \text{ and } e^{v_i} = \varepsilon\kappa \text{ for all } j \in J^\complement_{\varepsilon,\kappa},
\end{cases}
\end{align}
where 
\begin{align*}
\Psi_{\varepsilon,\kappa}(u, v) &:= \sum_{i\in I_{\varepsilon,\kappa}, j \in J_{\varepsilon,\kappa}}e^{u_i}K_{ij}e^{v_j} 
+ \varepsilon\kappa \sum_{i\in I_{\varepsilon,\kappa}, j\in J^\complement_{\varepsilon,\kappa}} e^{u_i}K_{ij} 
+ \varepsilon\kappa^{-1} \sum_{i \in I^\complement_{\varepsilon,\kappa}, j \in J_{\varepsilon,\kappa}} K_{ij}e^{v_j}\\
& \qquad - \kappa\sum_{i \in I_{\varepsilon,\kappa}}\mu_i u_i - \kappa^{-1} \sum_{j\in J_{\varepsilon,\kappa}} \nu_jv_j
+ \Xi,
\end{align*}
with $\Xi = \varepsilon^2 \sum_{i \in I^\complement_{\varepsilon,\kappa}, j \in J^\complement_{\varepsilon,\kappa}} K_{ij} -\kappa \log(\varepsilon\kappa^{-1})\sum_{i \in I^\complement_{\varepsilon,\kappa}}\mu_i - \kappa^{-1} \log(\varepsilon\kappa)\sum_{j\in J^\complement_{\varepsilon,\kappa}} \nu_j$.
We refer to problem~\eqref{screen-sinkhorn_second_def} as the \emph{screened dual of Sinkhorn divergence}.

\paragraph{First order conditions for problem~\eqref{screen-sinkhorn_second_def}.}

Let $(u^{\text{sc}}, v^{\text{sc}})$ be an optimal solution of problem~\eqref{screen-sinkhorn_second_def}, then we have 
\begin{align*}
e^{u^{\text{sc}}_i} \sum_{j\in J_{\varepsilon,\kappa}}K_{ij}e^{v^{\text{sc}}_j} + \varepsilon\kappa e^{u^{\text{sc}}_i}\sum_{j\in J^\complement_{\varepsilon,\kappa}}K_{ij} - \kappa \mu_i = 0, \text{ for all } i \in I_{\varepsilon,\kappa},
\end{align*}
and 
\begin{align*}
e^{v^{\text{sc}}_j} \sum_{i\in I_{\varepsilon,\kappa}}K_{ij}e^{u^{\text{sc}}_i} + \varepsilon\kappa^{-1} e^{v^{\text{sc}}_j}\sum_{i\in I^\complement_{\varepsilon,\kappa}}K_{ij} - \kappa^{-1} \nu_j = 0, \text{ for all } j \in J_{\varepsilon,\kappa}.
\end{align*}
Therefore
\begin{align*}
e^{u^{\text{sc}}_i} = \frac{\kappa\mu_i}{\sum_{j\in J_{\varepsilon,\kappa}}K_{ij}e^{v^{\text{sc}}_j} + \varepsilon\kappa \sum_{j\in J^\complement_{\varepsilon,\kappa}}K_{ij}}, \text{ for all } i \in I_{\varepsilon,\kappa},
\end{align*}
and 
\begin{align*}
e^{v^{\text{sc}}_i} = \frac{\kappa^{-1}\nu_j}{\sum_{i \in I_{\varepsilon,\kappa}}K_{ij}e^{u^{\text{sc}}_i} + \varepsilon\kappa^{-1} \sum_{i\in I^\complement_{\varepsilon,\kappa}}K_{ij}}, \text{ for all } j \in J_{\varepsilon,\kappa},
\end{align*}




\begin{lemma}
Let $(u^{\text{sc}}, v^{\text{sc}})$ be an optimal solution of problem~\eqref{screen-sinkhorn_second_def}. Then,
one has
\begin{equation}
\label{bound_on_u}
\frac \varepsilon\kappa \vee \frac{\min_{i \in I_{\kappa,\varepsilon}}\mu_i}{\varepsilon |J^\complement_{\varepsilon,\kappa}| + \varepsilon \vee \frac{\max_{j\in J_{\kappa,\varepsilon}} \nu_j}{n\varepsilon\kappa\min_{i,j}K_{ij}} |J_{\kappa,\varepsilon}|} \leq e^{u^{\text{sc}}_i} \leq \frac \varepsilon\kappa\vee \frac{\max_{i \in I_{\kappa,\varepsilon}} \mu_i}{m\varepsilon\min_{i,j}K_{ij}},
\end{equation}
and
\begin{equation}
\label{bound_on_v}
\varepsilon\kappa \vee \frac{\min_{j \in J_{\kappa,\varepsilon}}\nu_j}{\varepsilon|I^\complement_{\varepsilon,\kappa}| + \varepsilon \vee \frac{\kappa\max_{i\in I_{\kappa,\varepsilon}} \mu_i}{m\varepsilon\min_{i,j}K_{ij}} |I_{\kappa,\varepsilon}|} \leq e^{v^{\text{sc}}_j} \leq \varepsilon\kappa \vee \frac{\max_{j \in J_{\kappa,\varepsilon}} \nu_j}{n\varepsilon\min_{i,j}K_{ij}}
\end{equation}
for all $i\in I_{\kappa,\varepsilon}$ and $j\in J_{\kappa,\varepsilon}$.
\end{lemma}

\begin{proof}
We prove only the first statement~\eqref{bound_on_u} and symmetrically we can prove the second statement~\eqref{bound_on_v} for $e^{v^{\text{sc}}_j}$.
For all $i\in I_{\varepsilon,\kappa}$, we have $e^{u^{\text{sc}}_i} > \frac \varepsilon\kappa$ or $e^{u^{\text{sc}}_i} = \frac \varepsilon\kappa$. In one hand, if $e^{u^{\text{sc}}_i} > \frac \varepsilon\kappa$ then according to the optimality conditions $\lambda^{\text{sc}}_i = 0.$ Then $e^{u^{\text{sc}}_i} \sum_{j=1}^m K_{ij} e^{v^{\text{sc}}_j} = \kappa\mu_i$.
In another hand, we have 
\begin{align*}
e^{u^{\text{sc}}_i} \min_{i,j}K_{ij} \sum_{j=1}^m e^{v^{\text{sc}}_j} \leq e^{u^{\text{sc}}_i} \sum_{j=1}^m K_{ij} e^{v^{\text{sc}}_j} = \kappa\mu_i.
\end{align*}
We further observe that $\sum_{j=1}^m e^{v^{\text{sc}}_j} = \sum_{j \in J_{\kappa,\varepsilon}} e^{v^{\text{sc}}_j} + \sum_{j \in J^\complement_{\varepsilon,\kappa}} e^{v^{\text{sc}}_j} \geq \varepsilon\kappa |J_{\kappa,\varepsilon}| + \varepsilon\kappa |J^\complement_{\varepsilon,\kappa}| = \varepsilon\kappa (|J_{\kappa,\varepsilon}| + |J^\complement_{\varepsilon,\kappa}|) = \varepsilon\kappa m.$ Then
\begin{equation*}
\max_{i\in I_{\kappa,\varepsilon}} e^{u^{\text{sc}}_i} \leq \frac \varepsilon\kappa \vee \frac{\max_{i\in I_{\kappa,\varepsilon}}\mu_i}{m\varepsilon \min_{i,j}K_{ij}}.
\end{equation*}
Analogously, one can obtain for all $j\in J_{\kappa,\varepsilon}$
\begin{equation}
\label{upper_bound_v_potential}
\max_{j\in J_{\kappa,\varepsilon}}e^{v^{\text{sc}}_j} \leq \varepsilon\kappa \vee \frac{\max_{j \in J_{\kappa,\varepsilon}} \nu_j}{n\varepsilon\min_{i,j}K_{ij}}.
\end{equation}

Now, since $K_{ij} \leq 1$, we have 
\begin{align*}
e^{u^{\text{sc}}_i} \sum_{j=1}^m e^{v^{\text{sc}}_j} \geq e^{u^{\text{sc}}_i} \sum_{j=1}^m K_{ij}e^{v^{\text{sc}}_j} = \kappa\mu_i.
\end{align*}
Moreover, using~\eqref{upper_bound_v_potential}, we get 
\begin{align*}
\sum_{j=1}^m e^{v^{\text{sc}}_j} &= \sum_{j \in J_{\kappa,\varepsilon}} e^{v^{\text{sc}}_j} + \sum_{j \in J^\complement_{\varepsilon,\kappa}} e^{v^{\text{sc}}_j}
\leq \varepsilon\kappa |J^\complement_{\varepsilon,\kappa}| + \varepsilon\kappa \vee \frac{\max_{j\in J_{\kappa,\varepsilon}} \nu_j}{n\varepsilon\min_{i,j}K_{ij}} |J_{\kappa,\varepsilon}|.
\end{align*}
Therefore,
\begin{align*}
\min_{i \in I_{\kappa,\varepsilon}} e^{u^{\text{sc}}_i}  \geq \frac \varepsilon\kappa \vee \frac{\kappa\min_{I_{\kappa,\varepsilon}}\mu_i}{\varepsilon\kappa |J^\complement_{\varepsilon,\kappa}| + \varepsilon\kappa \vee \frac{\max_{j\in J_{\kappa,\varepsilon}} \nu_j}{n\varepsilon\min_{i,j}K_{ij}} |J_{\kappa,\varepsilon}|}.
\end{align*}
\end{proof}


\section{Numerical experiments}



% \subsubsection*{Acknowledgments}

% Use unnumbered third level headings for the acknowledgments. All acknowledgments go at the end of the paper. Do not include acknowledgments in the anonymized submission, only in the final paper.


\small
\bibliography{biblio}
\bibliographystyle{plain}

\end{document}




% \begin{remark}
% We define the maximal value $\varepsilon_{\max}$ as follows:
% \begin{align*}
% \label{varepsilon_max}
% \varepsilon_{\max} = \max\bigg\{\max_{i}\sqrt{\frac{\mu_i}{\sum_{j}K_{ij}}}, \max_{j}\sqrt{\frac{\nu_j}{\sum_{i}K_{ij}}}\bigg\}
% \end{align*}
% and 
% \begin{equation*}
% \label{varepsilon_min}
% \varepsilon_{\min} = \min\bigg\{\min_{i}\sqrt{\frac{\mu_i}{\sum_{j}K_{ij}}}, \min_{j}\sqrt{\frac{\nu_j}{\sum_{i}K_{ij}}}\bigg\}.
% \end{equation*}
% It implies that for a choice of $\varepsilon > \varepsilon_{\max}$, the couple solution of $(u^{*}, v^{*}) \equiv (\frac \varepsilon\kappa\mathbf{1}_n, \varepsilon\kappa \mathbf{1}_m)$. 
% Indeed, we obtain that $I_{{c,\varepsilon_{\max}}} = \varnothing$ and $J_{{c,\varepsilon_{\max}}} = \varnothing$. Moreover, there exists and $\varepsilon_{\min}$ for which $I_{c,\varepsilon_{\min}} = \{1, \ldots, n\}$ and $J_{c,\varepsilon_{\min}} = \{1, \ldots, m\}$.

% We remark that $\varepsilon_{\max} \nearrow \infty$ and $\varepsilon_{\min} \nearrow \infty$ as $\eta \rightarrow 0,$ and we have 
% \begin{equation*}
% \varepsilon_{\max}(\eta=\infty)= \max\bigg\{\max_{i}\sqrt{\frac{\mu_i}{m}}, \max_{j}\sqrt{\frac{\nu_j}{n}}\bigg\}.
% \end{equation*}
% and 
% \begin{equation*}
% \varepsilon_{\min}(\eta=\infty)= \min\bigg\{\min_{i}\sqrt{\frac{\mu_i}{m}}, \min_{j}\sqrt{\frac{\nu_j}{n}}\bigg\}.
% \end{equation*}
% \end{remark}

